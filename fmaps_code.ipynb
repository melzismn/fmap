{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fmaps.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/melzismn/fmap/blob/master/fmaps_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpI4m_FcmS2V"
      },
      "source": [
        "!pip install --upgrade --force-reinstall Pillow\n",
        "\n",
        "import igl\n",
        "import scipy\n",
        "import scipy as sp\n",
        "import numpy as np\n",
        "from meshplot import plot, subplot, interact\n",
        "import meshplot\n",
        "from scipy.sparse.linalg import eigs,eigsh\n",
        "from scipy.sparse import csr_matrix\n",
        "import os \n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rBPjw-0mXTL"
      },
      "source": [
        "# We will need to plot a pair of shapes\n",
        "def plot_pair(v1, v2, f1, f2, c1, c2, color_ops = {}):\n",
        "    # Compute a scale factor\n",
        "    M1 = igl.massmatrix(v1, f1, igl.MASSMATRIX_TYPE_VORONOI)\n",
        "    M2 = igl.massmatrix(v2, f2, igl.MASSMATRIX_TYPE_VORONOI)\n",
        "    scale_factor = np.sqrt(np.sum(M2)/np.sum(M1))\n",
        "\n",
        "    # Align the shapes\n",
        "    v2 = v2 - np.mean(v2,axis=0)\n",
        "    v1_align = v1 * scale_factor + np.mean(v1,axis=0) + [0.7,-0.7,0.0]\n",
        "\n",
        "    # Merge the models\n",
        "    v_all = np.vstack((v1_align, v2))\n",
        "    f_all = np.vstack((f1, f2 + np.max(f1)+1))\n",
        "    c_all = np.vstack((c1, c2))\n",
        "    \n",
        "    plot(v_all, f_all, c_all, shading = color_ops)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGY1lpZGai3v"
      },
      "source": [
        "# Load Shapes and compute the LBO eigendecomposition\n",
        "v_src, f_src = igl.read_triangle_mesh(os.path.join('.', \"data\", \"tr_reg_089.off\"))\n",
        "\n",
        "L_src = -igl.cotmatrix(v_src, f_src)\n",
        "M_src = igl.massmatrix(v_src, f_src, igl.MASSMATRIX_TYPE_VORONOI)\n",
        "\n",
        "try:\n",
        "    evals_src, evecs_src = eigsh(L_src, 200, M_src, sigma=0.0, which='LM', maxiter=1e9, tol=1.e-15)\n",
        "except:\n",
        "    evals_src, evecs_src = eigsh(L_src- 1e-8* scipy.sparse.identity(v_src.shape[0]), 200,\n",
        "                         M_src, sigma=0.0, which='LM', maxiter=1e9, tol=1.e-15)\n",
        "\n",
        "evals_src = evals_src.astype(np.float32)\n",
        "evals_src = evals_src.astype(np.float32)\n",
        "\n",
        "v_tar, f_tar = igl.read_triangle_mesh(os.path.join('.', \"data\", \"tr_reg_090.off\"))\n",
        "\n",
        "L_tar = -igl.cotmatrix(v_tar, f_tar)\n",
        "M_tar = igl.massmatrix(v_tar, f_tar, igl.MASSMATRIX_TYPE_VORONOI)\n",
        "\n",
        "try:\n",
        "    evals_tar, evecs_tar = eigsh(L_tar, 200, M_tar, sigma=0.0, which='LM', maxiter=1e9, tol=1.e-15)\n",
        "except:\n",
        "    evals_tar, evecs_tar = eigsh(L_tar- 1e-8* scipy.sparse.identity(v_tar.shape[0]), 200,\n",
        "                         M_src, sigma=0.0, which='LM', maxiter=1e9, tol=1.e-15)\n",
        "    \n",
        "evals_tar = evals_tar.astype(np.float32)\n",
        "evals_tar = evals_tar.astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xa35jY3hpwW5"
      },
      "source": [
        "# compute the WKS descriptor\n",
        "def WKS(vertices, faces, evals, evecs, wks_size, variance):\n",
        "    # Number of vertices\n",
        "    n = vertices.shape[0]\n",
        "    WKS = np.zeros((n,wks_size))\n",
        "\n",
        "    # Just for numerical stability\n",
        "    evals[evals<1e-6] = 1e-6\n",
        "\n",
        "    # log(E)\n",
        "    log_E = np.log(evals).T\n",
        "\n",
        "    # Define the energies step\n",
        "    e = np.linspace(log_E[1], np.max(log_E)/1.02, wks_size)\n",
        "\n",
        "    # Compute the sigma\n",
        "    sigma = (e[1]-e[0]) * variance\n",
        "    C = np.zeros((wks_size,1))\n",
        "\n",
        "    for i in np.arange(0,wks_size):\n",
        "        # Computing WKS\n",
        "        WKS[:,i] = np.sum(\n",
        "            (evecs)**2 * np.tile( np.exp((-(e[i] - log_E)**2) / (2*sigma**2)),(n,1)), axis=1)\n",
        "        \n",
        "        # Noramlization\n",
        "        C[i] = np.sum(np.exp((-(e[i]-log_E)**2)/(2*sigma**2)))\n",
        "        \n",
        "    WKS = np.divide(WKS,np.tile(C,(1,n)).T)\n",
        "    return WKS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBG2ZKlHp1Jv"
      },
      "source": [
        "# Computing the descriptors for the two shapes\n",
        "d_src = WKS(v_src, f_src, evals_src, evecs_src, 100, 7)\n",
        "d_tar = WKS(v_tar, f_tar, evals_tar, evecs_tar, 100, 7)\n",
        "\n",
        "# Visualizing descriptors\n",
        "i = 2\n",
        "plot_pair(v_src, v_tar, f_src, f_tar, d_src[:,i:i+1], d_tar[:,i:i+1])\n",
        "\n",
        "# Nearest Neighbor to obtain the p2p map\n",
        "treesearch = sp.spatial.cKDTree(d_tar)\n",
        "p2p = treesearch.query(d_src, k=1)[1]\n",
        "\n",
        "# To see the quality of the matching we plot a function on one shape and we transfer it to the other\n",
        "funz_ = (v_tar - np.min(v_tar,0))/np.tile((np.max(v_tar,0)-np.min(v_tar,0)),(np.size(v_tar,0),1));\n",
        "colors = np.cos(funz_);\n",
        "funz_tar = (colors-np.min(colors))/(np.max(colors) - np.min(colors));\n",
        "funz_src = funz_tar[p2p]\n",
        "\n",
        "# Compute and plot and (euclidean) error evaluation\n",
        "funz_src = funz_tar[p2p]\n",
        "plot_pair(v_src, v_tar, f_src, f_tar, funz_src,funz_tar)\n",
        "err = np.sum(np.square(v_tar - v_tar[p2p,:]))\n",
        "print(err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ttg0B3wy-VCq"
      },
      "source": [
        "# Functional Maps implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piGDB7xDAOnC"
      },
      "source": [
        "# Computing descriptors\n",
        "n_land =  10\n",
        "n_wks  =  20\n",
        "\n",
        "n_evals = 20\n",
        "\n",
        "# Landmarks\n",
        "step = np.int(np.ceil(v_src.shape[0] / n_land))\n",
        "a = np.arange(0,v_src.shape[0],step)\n",
        "landmarks = np.zeros((v_src.shape[0], a.size))\n",
        "landmarks[a,np.arange(a.size)] = 1\n",
        "\n",
        "# WKS\n",
        "d_src = WKS(v_src, f_src, evals_src, evecs_src, n_wks, 7)\n",
        "d_tar = WKS(v_tar, f_tar, evals_tar, evecs_tar, n_wks, 7)\n",
        "\n",
        "# Optimization Process\n",
        "desc_src = np.hstack((landmarks,d_src))\n",
        "desc_tar = np.hstack((landmarks,d_tar))\n",
        "\n",
        "# Descriptor normalization\n",
        "no = np.sqrt(np.diag(np.matmul(M_src.T.__matmul__(desc_src).T, desc_src)))\n",
        "no_s = np.tile(no.T,(v_src.shape[0],1))\n",
        "no_t = np.tile(no.T,(v_tar.shape[0],1))\n",
        "fct_src = np.divide(desc_src,no_s)\n",
        "fct_tar = np.divide(desc_tar,no_t)\n",
        "\n",
        "# Coefficents of the obtained descriptors\n",
        "Fct_src = np.matmul(M_src.T.__matmul__(evecs_src[:, 0:n_evals]).T, fct_src)\n",
        "Fct_tar = np.matmul(M_tar.T.__matmul__(evecs_tar[:, 0:n_evals]).T, fct_tar)\n",
        "\n",
        "# The relation between the two constant functions can be computed in a closed form\n",
        "constFct = np.zeros((n_evals,1))\n",
        "constFct[0, 0] = np.sign(evecs_src[0, 0] * evecs_tar[0, 0]) * np.sqrt(np.sum(M_tar)/np.sum(M_src))\n",
        "\n",
        "# Energy weights\n",
        "a = 1e-1 # Descriptors preservation\n",
        "c = 1e-8 # Commutativity with Laplacian\n",
        "\n",
        "# Define tensorflow objects\n",
        "fs = tf.constant(Fct_src, dtype=tf.float32)\n",
        "ft = tf.constant(Fct_tar, dtype=tf.float32)\n",
        "evals = tf.constant(tf.linalg.tensor_diag(np.reshape(np.float32(evals_src[0:n_evals]), (n_evals,))), dtype=tf.float32)\n",
        "evalt = tf.constant(tf.linalg.tensor_diag(np.reshape(np.float32(evals_tar[0:n_evals]), (n_evals,))), dtype=tf.float32)\n",
        "\n",
        "# Initialize C\n",
        "C_ini = np.zeros((n_evals,n_evals))\n",
        "C_ini[0,0]=constFct[0,0]\n",
        "C = tf.Variable(tf.zeros((n_evals,n_evals), dtype=tf.float32))\n",
        "C.assign(C_ini)\n",
        "\n",
        "# Optimizer\n",
        "adam = tf.keras.optimizers.Adam(1e-1) # Optimization technique\n",
        "trainable_vars = [C]\n",
        "\n",
        "# Optimization\n",
        "for i in np.arange(0,1000):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss1 = a * tf.reduce_sum(((tf.matmul(C, fs) - ft) ** 2)) / 2 # Descriptor preservation\n",
        "        loss2 = c * tf.reduce_sum((tf.matmul(C, evals) - tf.matmul(evalt,C))**2) #tf.reduce_sum(((C ** 2) * Dlb) / 2)  # Commute with Laplacian\n",
        "        #loss3 = 1e-6 *  tf.reduce_sum(tf.square(tf.matmul(tf.transpose(C),C) - tf.eye(n_evals))) # Orthonormal C\n",
        "        loss = loss1  + loss2# + loss3\n",
        "\n",
        "    # Apply gradient\n",
        "    grad = tape.gradient(loss,trainable_vars)\n",
        "    tmp = adam.apply_gradients(zip(grad,trainable_vars))\n",
        "\n",
        "C = C.numpy()\n",
        "plt.imshow(C)\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "treesearch = sp.spatial.cKDTree(np.matmul(evecs_tar[:,0:n_evals],C))\n",
        "p2p = treesearch.query(evecs_src[:,0:n_evals], k=1)[1]\n",
        "funz_src = funz_tar[p2p]\n",
        "plot_pair(v_src, v_tar, f_src, f_tar, funz_src,funz_tar)\n",
        "err = np.sum(np.square(v_src - v_src[p2p,:]))\n",
        "print(err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWbdO0ZWnNrn"
      },
      "source": [
        "# ICP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv-jXEQZ8BHC"
      },
      "source": [
        "print('ICP refine...')\n",
        "for k in np.arange(0,5):\n",
        "    treesearch = scipy.spatial.cKDTree(np.matmul(evecs_tar[:,0:n_evals],C))\n",
        "    matches = treesearch.query(evecs_src[:,0:n_evals], k=1)[1]\n",
        "    W = np.linalg.lstsq(evecs_src[matches, 0:n_evals],evecs_tar[:, 0:n_evals])[0]\n",
        "    d = np.linalg.svd(W)\n",
        "    C_ICP = np.matmul(np.matmul(d[0], np.eye(n_evals)), d[2])\n",
        "\n",
        "plt.imshow(C_ICP)\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "treesearch = sp.spatial.cKDTree(evecs_tar[:,0:n_evals])\n",
        "p2p = treesearch.query(np.matmul(evecs_src[:,0:n_evals], C_ICP), k=1)[1]\n",
        "funz_src = funz_tar[p2p]\n",
        "plot_pair(v_src, v_tar, f_src, f_tar, funz_src,funz_tar)\n",
        "err = np.sum(np.square(v_tar - v_tar[p2p,:]))\n",
        "print(err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcutF7d3UOdz"
      },
      "source": [
        "# ZoomOut refinement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQkuKj10UKld"
      },
      "source": [
        "# More iterations => bigger map => higher frequencies => better quality\n",
        "n_iter = 10\n",
        "\n",
        "# ZOOMOUT\n",
        "C_iter = C_ICP\n",
        "for i in np.arange(0,n_iter):\n",
        "  # 1) Convert into dense correspondence\n",
        "  treesearch = sp.spatial.cKDTree(np.matmul(evecs_tar[:,0:n_evals+i], C_iter.T))\n",
        "  p2p = treesearch.query(evecs_src[:,0:n_evals+i], k=1)[1]\n",
        "\n",
        "  #2) Convert into C of dimension (n+1) x (n+1)\n",
        "  C_iter = np.matmul(np.linalg.pinv(evecs_src[:,0:n_evals+i+1]),evecs_tar[p2p,0:n_evals+i+1])\n",
        "\n",
        "\n",
        "# Evaluate the map and visualize the result\n",
        "plt.imshow(C_iter)\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "treesearch = sp.spatial.cKDTree(np.matmul(evecs_tar[:,0:n_evals+n_iter],C_iter.T))\n",
        "p2p = treesearch.query(evecs_src[:,0:n_evals+n_iter], k=1)[1]\n",
        "funz_src = funz_tar[p2p]\n",
        "plot_pair(v_src, v_tar, f_src, f_tar, funz_src,funz_tar)\n",
        "err = np.sum(np.square(v_tar - v_tar[p2p,:]))\n",
        "print(err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYcTv7aeVMUy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}